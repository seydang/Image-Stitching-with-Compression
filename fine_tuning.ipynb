{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-YVC3I5kvSW"
      },
      "source": [
        "##setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "odG3WwFKnjOD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "outputId": "ce2616c8-b424-48ad-8f23-e32cdb18b240"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotImplementedError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-86-36bddcb12674>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install compressai'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# !pip install -i https://test.pypi.org/simple/ imgstitch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     97\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'also_return_output'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_system_commands\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_system_compat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint:disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpip_warn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_system_compat\u001b[0;34m(shell, cmd, also_return_output)\u001b[0m\n\u001b[1;32m    451\u001b[0m   \u001b[0;31m# is expected to call this function, thus adding one level of nesting to the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m   \u001b[0;31m# stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 453\u001b[0;31m   result = _run_command(\n\u001b[0m\u001b[1;32m    454\u001b[0m       \u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclear_streamed_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_run_command\u001b[0;34m(cmd, clear_streamed_output)\u001b[0m\n\u001b[1;32m    165\u001b[0m   \u001b[0mlocale_encoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlocale\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpreferredencoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mlocale_encoding\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0m_ENCODING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m     raise NotImplementedError(\n\u001b[0m\u001b[1;32m    168\u001b[0m         \u001b[0;34m'A UTF-8 locale is required. Got {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocale_encoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     )\n",
            "\u001b[0;31mNotImplementedError\u001b[0m: A UTF-8 locale is required. Got ANSI_X3.4-1968"
          ]
        }
      ],
      "source": [
        "%%capture\n",
        "!pip install compressai\n",
        "# !pip install -i https://test.pypi.org/simple/ imgstitch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YfIdFf5Oit5D"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import random\n",
        "import shutil\n",
        "import sys\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "# from compressai.datasets import ImageFolder\n",
        "from compressai.losses import RateDistortionLoss\n",
        "from compressai.optimizers import net_aux_optimizer\n",
        "from compressai.zoo import image_models\n",
        "from compressai.datasets import ImageFolder\n",
        "\n",
        "class AverageMeter:\n",
        "    \"\"\"Compute running average.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "\n",
        "class CustomDataParallel(nn.DataParallel):\n",
        "    \"\"\"Custom DataParallel to access the module methods.\"\"\"\n",
        "\n",
        "    def __getattr__(self, key):\n",
        "        try:\n",
        "            return super().__getattr__(key)\n",
        "        except AttributeError:\n",
        "            return getattr(self.module, key)\n",
        "\n",
        "\n",
        "def configure_optimizers(net, args):\n",
        "    \"\"\"Separate parameters for the main optimizer and the auxiliary optimizer.\n",
        "    Return two optimizers\"\"\"\n",
        "    conf = {\n",
        "        \"net\": {\"type\": \"Adam\", \"lr\": args.learning_rate},\n",
        "        \"aux\": {\"type\": \"Adam\", \"lr\": args.aux_learning_rate},\n",
        "    }\n",
        "    optimizer = net_aux_optimizer(net, conf)\n",
        "    return optimizer[\"net\"], optimizer[\"aux\"]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_5JaRssmYAwC",
        "outputId": "e6665a2f-33be-4340-f585-5e4629250181"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/MyDrive; to attempt to forcibly remount, call drive.mount(\"/content/MyDrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/MyDrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "\n",
        "def ozlem(ori_image):\n",
        "\n",
        "  image = ori_image[0].requires_grad_(requires_grad=True)\n",
        "  image = image.detach().cpu().numpy().transpose(1,2,0)\n",
        "  # image = image.detach().cpu().numpy()\n",
        "  # print(image.shape)\n",
        "\n",
        "  gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "  # print(gray_image.shape)\n",
        "\n",
        "  orb = cv2.ORB_create(nfeatures=2000)\n",
        "\n",
        "  # breakpoint()\n",
        "  kp, des = orb.detectAndCompute(image, None)\n",
        "\n",
        "  kp_image = cv2.drawKeypoints(np.uint8(image), kp, None, color=(0, 255, 0)).astype(np.float32)\n",
        "\n",
        "  kp_image = torch.from_numpy(np.asarray(kp_image)).to('cuda').requires_grad_(requires_grad=True)\n",
        "  # print(kp_image.size())\n",
        "\n",
        "  return kp_image"
      ],
      "metadata": {
        "id": "u_hM0J_hPpvB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img = cv2.imread('/content/MyDrive/MyDrive/inference_time/data/Kodak/train/1.png')\n",
        "img = torch.from_numpy(img)"
      ],
      "metadata": {
        "id": "rht5h2RCVPFt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EL3mPIBoOhN"
      },
      "source": [
        "##code"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = np.zeros((2,3))\n",
        "a = torch.from_numpy(a)"
      ],
      "metadata": {
        "id": "MPigg5KjbWBc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(a.size())\n",
        "print(a.unsqueeze(0).size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-xqu89gubZgA",
        "outputId": "6ec2403a-5189-42ae-ed99-fbd597294a21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3])\n",
            "torch.Size([1, 2, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZzJiUcb1VwUl"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(\n",
        "    model, criterion, train_dataloader, optimizer, aux_optimizer, epoch, clip_max_norm\n",
        "):\n",
        "    model.train()\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    for i, d in enumerate(train_dataloader):\n",
        "\n",
        "        image = d.to(device)\n",
        "        # target = d[1].to(device).to(torch.float32)\n",
        "        target = image\n",
        "        optimizer.zero_grad()\n",
        "        aux_optimizer.zero_grad()\n",
        "\n",
        "        out_net = model(image)\n",
        "        # print(ozlem(out_net['x_hat']).shape)\n",
        "        # print(ozlem(target).shape)\n",
        "\n",
        "\n",
        "        out_criterion_1 = criterion(out_net['x_hat'], target)\n",
        "        out_criterion_2 = criterion(ozlem(out_net['x_hat']).unsqueeze(0), ozlem(target).unsqueeze(0))\n",
        "\n",
        "        out_criterion_2[\"loss\"].backward()\n",
        "        if clip_max_norm > 0:\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip_max_norm)\n",
        "        optimizer.step()\n",
        "\n",
        "        aux_loss = model.aux_loss()\n",
        "        aux_loss.backward()\n",
        "        aux_optimizer.step()\n",
        "\n",
        "        if i % 4 == 0:\n",
        "            print(\n",
        "                f\"Train epoch {epoch}: [\"\n",
        "                f\"{i*len(d)}/{len(train_dataloader.dataset)}\"\n",
        "                f\" ({100. * i / len(train_dataloader):.0f}%)]\"\n",
        "                f'\\tMSE loss: {out_criterion_1[\"loss\"].item():.3f} |'\n",
        "                f'\\tMSE loss: {out_criterion_2[\"loss\"].item():.3f} |'\n",
        "                f\"\\tAux loss: {aux_loss.item():.2f}\"\n",
        "            )\n",
        "\n",
        "    # return out_criterion_2[\"mse_loss\"].item(), out_criterion[\"bpp_loss\"].item(), out_criterion[\"loss\"].item()\n",
        "\n",
        "\n",
        "def test_epoch(epoch, test_dataloader, model, criterion):\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    loss = AverageMeter()\n",
        "    bpp_loss = AverageMeter()\n",
        "    mse_loss = AverageMeter()\n",
        "    aux_loss = AverageMeter()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for d in test_dataloader:\n",
        "            image = d[0].to(device)\n",
        "            target = d[1].to(device)\n",
        "            out_net = model(image)\n",
        "            out_criterion = criterion(out_net, target)\n",
        "\n",
        "            aux_loss.update(model.aux_loss())\n",
        "            bpp_loss.update(out_criterion[\"bpp_loss\"])\n",
        "            loss.update(out_criterion[\"loss\"])\n",
        "            mse_loss.update(out_criterion[\"mse_loss\"])\n",
        "\n",
        "    print(\n",
        "        f\"Test epoch {epoch}: Average losses:\"\n",
        "        f\"\\tLoss: {loss.avg:.3f} |\"\n",
        "        f\"\\tMSE loss: {mse_loss.avg:.3f} |\"\n",
        "        f\"\\tBpp loss: {bpp_loss.avg:.2f} |\"\n",
        "        f\"\\tAux loss: {aux_loss.avg:.2f}\\n\"\n",
        "    )\n",
        "\n",
        "    return loss.avg\n",
        "\n",
        "\n",
        "def save_checkpoint(state, is_best, filename=\"checkpoint.pth.tar\"):\n",
        "    torch.save(state, filename)\n",
        "    if is_best:\n",
        "        shutil.copyfile(filename, \"checkpoint_best_loss.pth.tar\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class RateDistortionLoss(nn.Module):\n",
        "    \"\"\"Custom rate distortion loss with a Lagrangian parameter.\"\"\"\n",
        "\n",
        "    def __init__(self, lmbda=0.01, metric=\"mse\", return_type=\"all\"):\n",
        "        super().__init__()\n",
        "\n",
        "        self.metric = nn.MSELoss()\n",
        "\n",
        "        self.lmbda = lmbda\n",
        "        self.return_type = return_type\n",
        "\n",
        "    def forward(self, output, target):\n",
        "        N, _, H, W = target.size()\n",
        "        out = {}\n",
        "        num_pixels = N * H * W\n",
        "\n",
        "        # out[\"bpp_loss\"] = sum(\n",
        "        #     (torch.log(likelihoods).sum() / (-math.log(2) * num_pixels))\n",
        "        #     for likelihoods in output[\"likelihoods\"].values()\n",
        "        # )\n",
        "\n",
        "        # out[\"mse_loss\"] = self.metric(output[\"x_hat\"], target)\n",
        "        out[\"loss\"] = self.metric(output, target)* 255**2\n",
        "        \n",
        "        # out[\"loss\"] = self.lmbda * distortion + out[\"bpp_loss\"]\n",
        "        return out"
      ],
      "metadata": {
        "id": "i17b5gYpMfDR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCJhhlWBVvPV"
      },
      "source": [
        "##main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F69sxmbknlZI"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "\n",
        "    class arguments:\n",
        "      def __init__(self):\n",
        "        self.model = 'bmshj2018-hyperprior'\n",
        "        self.num_workers = 1\n",
        "        self.dataset = '/content/MyDrive/MyDrive/inference_time/data/Kodak'\n",
        "        self.batch_size = 1\n",
        "        self.test_batch_size = 2\n",
        "        self.cuda = True\n",
        "        self.epochs = 50\n",
        "        self.patch_size = 64\n",
        "        self.learning_rate = 0.001\n",
        "        self.aux_learning_rate = 0.001\n",
        "        self.lmbda = 0.001\n",
        "        self.lmbda2 = 1\n",
        "        self.save = True\n",
        "        self.seed = False\n",
        "        self.clip_max_norm = 255.0\n",
        "        self.checkpoint = False\n",
        "\n",
        "    args = arguments()\n",
        "\n",
        "    if args.seed is not None:\n",
        "        torch.manual_seed(args.seed)\n",
        "        random.seed(args.seed)\n",
        "\n",
        "    train_transforms = transforms.Compose(\n",
        "        [transforms.ToTensor()]\n",
        "    )\n",
        "\n",
        "    test_transforms = transforms.Compose(\n",
        "        [transforms.ToTensor()]\n",
        "    )\n",
        "\n",
        "    train_dataset = ImageFolder(args.dataset, split=\"train\", transform=train_transforms)\n",
        "    # test_dataset = ImageFolder(args.dataset, split=\"\", transform=test_transforms)\n",
        "\n",
        "    device = \"cuda\" if args.cuda and torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    train_dataloader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=args.batch_size,\n",
        "        num_workers=args.num_workers,\n",
        "        shuffle=True,\n",
        "        pin_memory=(device == \"cuda\"),\n",
        "    )\n",
        "\n",
        "    # test_dataloader = DataLoader(\n",
        "    #     test_dataset,\n",
        "    #     batch_size=args.test_batch_size,\n",
        "    #     num_workers=args.num_workers,\n",
        "    #     shuffle=False,\n",
        "    #     pin_memory=(device == \"cuda\"),\n",
        "    # )\n",
        "\n",
        "    net = image_models[args.model](quality=3)\n",
        "    ############## to give 5 different images:\n",
        "    # net.g_a[0] = torch.nn.Conv2d(15, 128, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2)) \n",
        "    ##########################################\n",
        "    net = net.to(device)\n",
        "\n",
        "    if args.cuda and torch.cuda.device_count() > 1:\n",
        "        net = CustomDataParallel(net)\n",
        "\n",
        "    optimizer, aux_optimizer = configure_optimizers(net, args)\n",
        "    lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, \"min\")\n",
        "    criterion = RateDistortionLoss(lmbda=args.lmbda)\n",
        "\n",
        "    last_epoch = 0\n",
        "    if args.checkpoint:  # load from previous checkpoint\n",
        "        print(\"Loading\", args.checkpoint)\n",
        "        checkpoint = torch.load(args.checkpoint, map_location=device)\n",
        "        last_epoch = checkpoint[\"epoch\"] + 1\n",
        "        net.load_state_dict(checkpoint[\"state_dict\"])\n",
        "        optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
        "        aux_optimizer.load_state_dict(checkpoint[\"aux_optimizer\"])\n",
        "        lr_scheduler.load_state_dict(checkpoint[\"lr_scheduler\"])\n",
        "\n",
        "    best_loss = float(\"inf\")\n",
        "    mse_loss = np.zeros((args.epochs,1))\n",
        "    bpp_loss = np.zeros((args.epochs,1))\n",
        "    loss = np.zeros((args.epochs,1))\n",
        "    for epoch in range(last_epoch, args.epochs):\n",
        "        if epoch%5==0: optimizer.param_groups[0]['lr'] /= 2\n",
        "        print(f\"\\nLearning rate: {optimizer.param_groups[0]['lr']}\")\n",
        "        # mse_loss[epoch], bpp_loss[epoch], loss[epoch] = train_one_epoch(\n",
        "        train_one_epoch(\n",
        "            net,\n",
        "            criterion,\n",
        "            train_dataloader,\n",
        "            optimizer,\n",
        "            aux_optimizer,\n",
        "            epoch,\n",
        "            args.clip_max_norm,\n",
        "        )\n",
        "        # loss = test_epoch(epoch, test_dataloader, net, criterion)\n",
        "        # lr_scheduler.step(loss)\n",
        "\n",
        "        # is_best = loss < best_loss\n",
        "        # best_loss = min(loss, best_loss)\n",
        "\n",
        "        if args.save:\n",
        "            save_checkpoint(\n",
        "                {\n",
        "                    \"epoch\": epoch,\n",
        "                    \"state_dict\": net.state_dict(),\n",
        "                    # \"loss\": loss,\n",
        "                    \"optimizer\": optimizer.state_dict(),\n",
        "                    \"aux_optimizer\": aux_optimizer.state_dict(),\n",
        "                    \"lr_scheduler\": lr_scheduler.state_dict(),\n",
        "                },\n",
        "                is_best=0,\n",
        "            )\n",
        "    return mse_loss, bpp_loss, loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwuq5Rz8X3Ae"
      },
      "source": [
        "##train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DiFiN4tZX2R5",
        "outputId": "6b1d5963-72b9-4e4e-e683-cc36588bb661"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Learning rate: 0.0005\n",
            "Train epoch 0: [0/24 (0%)]\tMSE loss: 0.631 |\tMSE loss: 1858.163 |\tAux loss: 5277.59\n",
            "Train epoch 0: [4/24 (17%)]\tMSE loss: 0.124 |\tMSE loss: 0.007 |\tAux loss: 5277.44\n",
            "Train epoch 0: [8/24 (33%)]\tMSE loss: 0.169 |\tMSE loss: 0.006 |\tAux loss: 5277.28\n",
            "Train epoch 0: [12/24 (50%)]\tMSE loss: 0.461 |\tMSE loss: 0.018 |\tAux loss: 5277.13\n",
            "Train epoch 0: [16/24 (67%)]\tMSE loss: 0.273 |\tMSE loss: 0.004 |\tAux loss: 5276.98\n",
            "Train epoch 0: [20/24 (83%)]\tMSE loss: 0.324 |\tMSE loss: 0.052 |\tAux loss: 5276.82\n",
            "\n",
            "Learning rate: 0.0005\n",
            "Train epoch 1: [0/24 (0%)]\tMSE loss: 0.244 |\tMSE loss: 0.020 |\tAux loss: 5276.67\n",
            "Train epoch 1: [4/24 (17%)]\tMSE loss: 0.199 |\tMSE loss: 0.001 |\tAux loss: 5276.52\n",
            "Train epoch 1: [8/24 (33%)]\tMSE loss: 0.225 |\tMSE loss: 0.002 |\tAux loss: 5276.36\n",
            "Train epoch 1: [12/24 (50%)]\tMSE loss: 0.188 |\tMSE loss: 0.001 |\tAux loss: 5276.21\n",
            "Train epoch 1: [16/24 (67%)]\tMSE loss: 0.240 |\tMSE loss: 0.001 |\tAux loss: 5276.06\n",
            "Train epoch 1: [20/24 (83%)]\tMSE loss: 0.157 |\tMSE loss: 0.001 |\tAux loss: 5275.90\n",
            "\n",
            "Learning rate: 0.0005\n",
            "Train epoch 2: [0/24 (0%)]\tMSE loss: 0.273 |\tMSE loss: 0.004 |\tAux loss: 5275.75\n",
            "Train epoch 2: [4/24 (17%)]\tMSE loss: 0.631 |\tMSE loss: 0.343 |\tAux loss: 5275.60\n",
            "Train epoch 2: [8/24 (33%)]\tMSE loss: 0.169 |\tMSE loss: 0.006 |\tAux loss: 5275.44\n",
            "Train epoch 2: [12/24 (50%)]\tMSE loss: 0.349 |\tMSE loss: 0.051 |\tAux loss: 5275.29\n",
            "Train epoch 2: [16/24 (67%)]\tMSE loss: 0.258 |\tMSE loss: 0.061 |\tAux loss: 5275.14\n",
            "Train epoch 2: [20/24 (83%)]\tMSE loss: 0.214 |\tMSE loss: 0.006 |\tAux loss: 5274.98\n",
            "\n",
            "Learning rate: 0.0005\n",
            "Train epoch 3: [0/24 (0%)]\tMSE loss: 0.188 |\tMSE loss: 0.001 |\tAux loss: 5274.83\n",
            "Train epoch 3: [4/24 (17%)]\tMSE loss: 0.335 |\tMSE loss: 0.032 |\tAux loss: 5274.68\n",
            "Train epoch 3: [8/24 (33%)]\tMSE loss: 0.631 |\tMSE loss: 0.343 |\tAux loss: 5274.52\n",
            "Train epoch 3: [12/24 (50%)]\tMSE loss: 0.157 |\tMSE loss: 0.001 |\tAux loss: 5274.37\n",
            "Train epoch 3: [16/24 (67%)]\tMSE loss: 0.251 |\tMSE loss: 0.009 |\tAux loss: 5274.22\n",
            "Train epoch 3: [20/24 (83%)]\tMSE loss: 0.461 |\tMSE loss: 0.018 |\tAux loss: 5274.06\n",
            "\n",
            "Learning rate: 0.0005\n",
            "Train epoch 4: [0/24 (0%)]\tMSE loss: 0.330 |\tMSE loss: 0.001 |\tAux loss: 5273.91\n",
            "Train epoch 4: [4/24 (17%)]\tMSE loss: 0.199 |\tMSE loss: 0.001 |\tAux loss: 5273.76\n",
            "Train epoch 4: [8/24 (33%)]\tMSE loss: 0.157 |\tMSE loss: 0.001 |\tAux loss: 5273.60\n",
            "Train epoch 4: [12/24 (50%)]\tMSE loss: 0.252 |\tMSE loss: 0.012 |\tAux loss: 5273.45\n",
            "Train epoch 4: [16/24 (67%)]\tMSE loss: 0.631 |\tMSE loss: 0.343 |\tAux loss: 5273.30\n",
            "Train epoch 4: [20/24 (83%)]\tMSE loss: 0.176 |\tMSE loss: 0.000 |\tAux loss: 5273.14\n",
            "\n",
            "Learning rate: 0.00025\n",
            "Train epoch 5: [0/24 (0%)]\tMSE loss: 0.287 |\tMSE loss: 0.007 |\tAux loss: 5272.99\n",
            "Train epoch 5: [4/24 (17%)]\tMSE loss: 0.124 |\tMSE loss: 0.007 |\tAux loss: 5272.84\n",
            "Train epoch 5: [8/24 (33%)]\tMSE loss: 0.199 |\tMSE loss: 0.001 |\tAux loss: 5272.68\n",
            "Train epoch 5: [12/24 (50%)]\tMSE loss: 0.461 |\tMSE loss: 0.018 |\tAux loss: 5272.53\n",
            "Train epoch 5: [16/24 (67%)]\tMSE loss: 0.273 |\tMSE loss: 0.004 |\tAux loss: 5272.38\n",
            "Train epoch 5: [20/24 (83%)]\tMSE loss: 0.244 |\tMSE loss: 0.020 |\tAux loss: 5272.23\n",
            "\n",
            "Learning rate: 0.00025\n",
            "Train epoch 6: [0/24 (0%)]\tMSE loss: 0.631 |\tMSE loss: 0.343 |\tAux loss: 5272.07\n",
            "Train epoch 6: [4/24 (17%)]\tMSE loss: 0.244 |\tMSE loss: 0.020 |\tAux loss: 5271.92\n",
            "Train epoch 6: [8/24 (33%)]\tMSE loss: 0.252 |\tMSE loss: 0.012 |\tAux loss: 5271.77\n",
            "Train epoch 6: [12/24 (50%)]\tMSE loss: 0.349 |\tMSE loss: 0.051 |\tAux loss: 5271.61\n",
            "Train epoch 6: [16/24 (67%)]\tMSE loss: 0.240 |\tMSE loss: 0.001 |\tAux loss: 5271.46\n",
            "Train epoch 6: [20/24 (83%)]\tMSE loss: 0.188 |\tMSE loss: 0.001 |\tAux loss: 5271.31\n",
            "\n",
            "Learning rate: 0.00025\n",
            "Train epoch 7: [0/24 (0%)]\tMSE loss: 0.157 |\tMSE loss: 0.001 |\tAux loss: 5271.16\n",
            "Train epoch 7: [4/24 (17%)]\tMSE loss: 0.251 |\tMSE loss: 0.009 |\tAux loss: 5271.00\n",
            "Train epoch 7: [8/24 (33%)]\tMSE loss: 0.234 |\tMSE loss: 0.001 |\tAux loss: 5270.85\n",
            "Train epoch 7: [12/24 (50%)]\tMSE loss: 0.252 |\tMSE loss: 0.012 |\tAux loss: 5270.70\n",
            "Train epoch 7: [16/24 (67%)]\tMSE loss: 0.244 |\tMSE loss: 0.020 |\tAux loss: 5270.55\n",
            "Train epoch 7: [20/24 (83%)]\tMSE loss: 0.240 |\tMSE loss: 0.001 |\tAux loss: 5270.39\n",
            "\n",
            "Learning rate: 0.00025\n",
            "Train epoch 8: [0/24 (0%)]\tMSE loss: 0.188 |\tMSE loss: 0.001 |\tAux loss: 5270.24\n",
            "Train epoch 8: [4/24 (17%)]\tMSE loss: 0.252 |\tMSE loss: 0.012 |\tAux loss: 5270.09\n",
            "Train epoch 8: [8/24 (33%)]\tMSE loss: 0.235 |\tMSE loss: 0.001 |\tAux loss: 5269.94\n",
            "Train epoch 8: [12/24 (50%)]\tMSE loss: 0.349 |\tMSE loss: 0.051 |\tAux loss: 5269.78\n",
            "Train epoch 8: [16/24 (67%)]\tMSE loss: 0.199 |\tMSE loss: 0.001 |\tAux loss: 5269.63\n",
            "Train epoch 8: [20/24 (83%)]\tMSE loss: 0.273 |\tMSE loss: 0.004 |\tAux loss: 5269.48\n",
            "\n",
            "Learning rate: 0.00025\n",
            "Train epoch 9: [0/24 (0%)]\tMSE loss: 0.124 |\tMSE loss: 0.007 |\tAux loss: 5269.33\n",
            "Train epoch 9: [4/24 (17%)]\tMSE loss: 0.214 |\tMSE loss: 0.006 |\tAux loss: 5269.17\n",
            "Train epoch 9: [8/24 (33%)]\tMSE loss: 0.240 |\tMSE loss: 0.001 |\tAux loss: 5269.02\n",
            "Train epoch 9: [12/24 (50%)]\tMSE loss: 0.199 |\tMSE loss: 0.001 |\tAux loss: 5268.87\n",
            "Train epoch 9: [16/24 (67%)]\tMSE loss: 0.349 |\tMSE loss: 0.051 |\tAux loss: 5268.72\n",
            "Train epoch 9: [20/24 (83%)]\tMSE loss: 0.244 |\tMSE loss: 0.020 |\tAux loss: 5268.56\n",
            "\n",
            "Learning rate: 0.000125\n",
            "Train epoch 10: [0/24 (0%)]\tMSE loss: 0.188 |\tMSE loss: 0.001 |\tAux loss: 5268.41\n",
            "Train epoch 10: [4/24 (17%)]\tMSE loss: 0.234 |\tMSE loss: 0.001 |\tAux loss: 5268.26\n",
            "Train epoch 10: [8/24 (33%)]\tMSE loss: 0.157 |\tMSE loss: 0.001 |\tAux loss: 5268.11\n",
            "Train epoch 10: [12/24 (50%)]\tMSE loss: 0.228 |\tMSE loss: 0.001 |\tAux loss: 5267.95\n",
            "Train epoch 10: [16/24 (67%)]\tMSE loss: 0.176 |\tMSE loss: 0.000 |\tAux loss: 5267.80\n",
            "Train epoch 10: [20/24 (83%)]\tMSE loss: 0.251 |\tMSE loss: 0.009 |\tAux loss: 5267.65\n",
            "\n",
            "Learning rate: 0.000125\n"
          ]
        }
      ],
      "source": [
        "main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZeRTEx8QWsen"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "t-YVC3I5kvSW",
        "6EL3mPIBoOhN"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}